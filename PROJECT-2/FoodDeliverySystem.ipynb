{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "147ac295",
      "metadata": {
        "id": "147ac295"
      },
      "source": [
        "# Food Delivery Customer Segmentation Analysis\n",
        "\n",
        "Welcome to this notebook, which details a customer segmentation project for a food delivery service. We will:\n",
        "\n",
        "-   Load and preprocess customer data.\n",
        "-   Apply various clustering algorithms (K-Means, Agglomerative, DBSCAN) to segment customers.\n",
        "-   Visualize clusters and derive insights for business strategies.\n",
        "\n",
        "## 1. Goal and Objective\n",
        "\n",
        "### Goal\n",
        "\n",
        "To identify distinct customer segments within a food delivery dataset based on behavioral and demographic attributes, enabling targeted marketing and personalized service.\n",
        "\n",
        "### Objective\n",
        "\n",
        "*   **Data Preparation:** Explore, clean, and preprocess the dataset (encoding, scaling, dimensionality reduction).\n",
        "*   **Clustering:** Apply K-Means, Agglomerative Clustering, and DBSCAN to segment customers.\n",
        "*   **Visualization & Insights:** Visualize clusters, interpret results, and present key observations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b216557",
      "metadata": {
        "id": "5b216557"
      },
      "source": [
        "## 2. Importing Libraries and Loading Data\n",
        "\n",
        "### Importing Required Libraries\n",
        "\n",
        "Essential Python libraries for data manipulation, visualization, and machine learning are imported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "029dad24",
      "metadata": {
        "id": "029dad24"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as dendrogram\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f46806b",
      "metadata": {
        "id": "3f46806b"
      },
      "source": [
        "### Dataset Overview\n",
        "\n",
        "The `food_delivery.csv` dataset is loaded and its first few rows are displayed for an initial review of its structure and content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fa2aef8",
      "metadata": {
        "id": "8fa2aef8"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv('food_delivery.csv')\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e6cc20a",
      "metadata": {
        "id": "6e6cc20a"
      },
      "source": [
        "## 3. Basic Dataset Information\n",
        "\n",
        "### Basic Information and Data Quality Check\n",
        "\n",
        "This section verifies the dataset's dimensions, checks for missing values, and identifies duplicate records to ensure data quality.\n",
        "\n",
        "Check the shape of the DataFrame (rows, columns):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf187d6",
      "metadata": {
        "id": "8cf187d6"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a15f7a05",
      "metadata": {
        "id": "a15f7a05"
      },
      "source": [
        "**Outcome:** `(500, 7)` confirms 500 rows and 7 columns.\n",
        "\n",
        "Check for null values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7059eb7",
      "metadata": {
        "id": "e7059eb7"
      },
      "outputs": [],
      "source": [
        "# Check for null values\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f94d8b2",
      "metadata": {
        "id": "5f94d8b2"
      },
      "source": [
        "**Outcome:** All columns show `0` null values, indicating a complete dataset.\n",
        "\n",
        "Check for duplicate rows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc62452",
      "metadata": {
        "id": "acc62452"
      },
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c113c96a",
      "metadata": {
        "id": "c113c96a"
      },
      "source": [
        "**Outcome:** `0` duplicates found, confirming no redundant records."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "128e2b32",
      "metadata": {
        "id": "128e2b32"
      },
      "source": [
        "### Descriptive Statistics\n",
        "\n",
        "Descriptive statistics for numerical columns are generated to understand their distribution, central tendency, and spread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf6a058",
      "metadata": {
        "id": "4bf6a058"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21771ae8",
      "metadata": {
        "id": "21771ae8"
      },
      "source": [
        "## Visualizing Feature Distributions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e5373d9",
      "metadata": {
        "id": "5e5373d9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# List of numerical columns to plot (excluding UserID and encoded fav_cuisine for now)\n",
        "numerical_cols = ['Age', 'TotalOrders', 'AverageSpend', 'AppUsageTimePerDay', 'DeliveryRating']\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    plt.subplot(2, 3, i + 1) # Adjust subplot grid based on number of columns\n",
        "    sns.histplot(df[col], kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    sns.boxplot(y=df[col]) # You can also add box plots for outlier detection\n",
        "    plt.title(f'Box Plot of {col}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1bcce5",
      "metadata": {
        "id": "9b1bcce5"
      },
      "outputs": [],
      "source": [
        "# Pair Plot for overall relationships and potential initial clusters\n",
        "# This can be heavy for many features, but useful for understanding interactions\n",
        "# Consider using data1 directly, as FavoriteCuisine is now encoded\n",
        "\n",
        "print(\"\\nPair Plot of Numerical Features:\")\n",
        "sns.pairplot(df.drop('UserID', axis=1), hue='FavoriteCuisine', palette='viridis') # Dropping UserID and using fav_cuisine for hue\n",
        "plt.suptitle('Pair Plot of Customer Features', y=1.02) # Adjust title position\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df09875e",
      "metadata": {
        "id": "df09875e"
      },
      "source": [
        "## 4. Data Preprocessing\n",
        "\n",
        "### Feature Encoding and DataFrame Preparation\n",
        "\n",
        "The categorical `FavoriteCuisine` column is converted into numerical labels using `LabelEncoder`. The original `FavoriteCuisine` column is then dropped, creating `data1` which includes the new `fav_cuisine` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a580f512",
      "metadata": {
        "id": "a580f512"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "df['fav_cuisine'] = le.fit_transform(df['FavoriteCuisine'])\n",
        "data1 = df.drop(['FavoriteCuisine'],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "496d475e",
      "metadata": {
        "id": "496d475e"
      },
      "source": [
        "### Distribution of Encoded `fav_cuisine`\n",
        "\n",
        "The frequency of each encoded cuisine type is examined to understand customer preferences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6571eba",
      "metadata": {
        "id": "e6571eba"
      },
      "outputs": [],
      "source": [
        "data1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "185f67e6",
      "metadata": {
        "id": "185f67e6"
      },
      "outputs": [],
      "source": [
        "data1['fav_cuisine'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38fb94c2",
      "metadata": {
        "id": "38fb94c2"
      },
      "source": [
        "**Outcome:** value_counts() shows varying popularity among cuisines (e.g., type 4 most frequent with 108, type 0 least frequent with 87), indicating uneven distribution of customer preferences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa4b9ab",
      "metadata": {
        "id": "5aa4b9ab"
      },
      "source": [
        "## 5. Bivariate/Multivariate Analysis\n",
        "\n",
        "### Correlation Matrix Heatmap\n",
        "\n",
        "A heatmap visualizing the Pearson correlation coefficients between numerical features in `data1` is generated. This helps in understanding linear relationships between variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f0cf5f2",
      "metadata": {
        "id": "4f0cf5f2"
      },
      "outputs": [],
      "source": [
        "corr=data1.corr()\n",
        "corr\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.heatmap(corr,annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d74fb050",
      "metadata": {
        "id": "d74fb050"
      },
      "source": [
        "**Outcome**: The heatmap shows strong positive correlation between TotalOrders and AverageSpend. Other correlations are low to moderate, suggesting features provide unique insights for clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a72e4990",
      "metadata": {
        "id": "a72e4990"
      },
      "outputs": [],
      "source": [
        "X = data1.drop('UserID',axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88386c94",
      "metadata": {
        "id": "88386c94"
      },
      "source": [
        "## 6. Feature Scaling and Dimensionality Reduction\n",
        "\n",
        "### Feature Scaling\n",
        "\n",
        "Before applying most clustering algorithms, it is crucial to scale numerical features. This prevents features with larger numerical ranges (like `AverageSpend`) from disproportionately influencing distance calculations compared to features with smaller ranges (like `DeliveryRating`). `StandardScaler` transforms the data so each feature has a mean of 0 and a standard deviation of 1. The `UserID` column is dropped as it's an identifier, not a feature for clustering.\n",
        "\n",
        "### explanation of work done\n",
        "\n",
        "The first line creates X by removing UserID from data1, leaving only the features relevant for clustering. StandardScaler is initialized and then fit_transform is applied to X, standardizing all its numerical columns. X_Scaled now holds the processed, scaled features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7235b60",
      "metadata": {
        "id": "c7235b60"
      },
      "outputs": [],
      "source": [
        "sc=StandardScaler()\n",
        "\n",
        "X_Scaled=sc.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e5482d",
      "metadata": {
        "id": "c8e5482d"
      },
      "source": [
        "### Dimensionality Reduction using PCA\n",
        "\n",
        "Following scaling, Principal Component Analysis (PCA) is applied to reduce the dimensionality of the dataset. This transformation projects the data into a lower-dimensional space while retaining most of its variance, which is essential for visualizing high-dimensional clusters in 2D plots. Here, `n_components=2` is chosen to reduce the data to two principal components."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70931728",
      "metadata": {
        "id": "70931728"
      },
      "source": [
        "**Explanation:** PCA(n_components=2) initializes PCA to extract the top 2 principal components. pca.fit_transform(X_Scaled) then applies this transformation to the scaled data, reducing its dimensions. X_pca now contains the data represented by these two principal components, which are orthogonal and capture the most variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "127d70bb",
      "metadata": {
        "id": "127d70bb"
      },
      "outputs": [],
      "source": [
        "pca=PCA(n_components=2)\n",
        "X_pca=pca.fit_transform(X_Scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b210ff11",
      "metadata": {
        "id": "b210ff11"
      },
      "source": [
        "## Effect of Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbd48a74",
      "metadata": {
        "id": "fbd48a74"
      },
      "outputs": [],
      "source": [
        "# Select a feature to visualize before and after scaling (e.g., AverageSpend, as it often has a wide range)\n",
        "feature_name = 'AverageSpend'\n",
        "feature_index = X.columns.get_loc(feature_name) # Get index of the feature in X\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(X[feature_name], kde=True)\n",
        "plt.title(f'Original Distribution of {feature_name}')\n",
        "plt.xlabel(feature_name)\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(X_Scaled[:, feature_index], kde=True)\n",
        "plt.title(f'Scaled Distribution of {feature_name}')\n",
        "plt.xlabel(f'{feature_name} (Scaled)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ca02290",
      "metadata": {
        "id": "6ca02290"
      },
      "outputs": [],
      "source": [
        "print(f\"Mean of original {feature_name}: {X[feature_name].mean():.2f}\")\n",
        "print(f\"Std Dev of original {feature_name}: {X[feature_name].std():.2f}\")\n",
        "print(f\"Mean of scaled {feature_name}: {X_Scaled[:, feature_index].mean():.2f}\")\n",
        "print(f\"Std Dev of scaled {feature_name}: {X_Scaled[:, feature_index].std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f40a48",
      "metadata": {
        "id": "67f40a48"
      },
      "source": [
        "## Explained Variance Ratio by Principal Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86100c9e",
      "metadata": {
        "id": "86100c9e"
      },
      "outputs": [],
      "source": [
        "# Re-run PCA without limiting n_components to see full variance explained\n",
        "pca_full = PCA()\n",
        "pca_full.fit(X_Scaled)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance Ratio by PCA Components')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Variance explained by first 2 components: {np.sum(pca_full.explained_variance_ratio_[:2]):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2fe877b",
      "metadata": {
        "id": "a2fe877b"
      },
      "source": [
        "## 7. Clustering Algorithms\n",
        "\n",
        "This section applies various unsupervised clustering algorithms to the prepared and dimensionality-reduced data.\n",
        "\n",
        "### K-Means Clustering\n",
        "\n",
        "K-Means is a popular centroid-based clustering algorithm that aims to partition observations into a predefined number of `k` clusters. Here, we apply K-Means with `n_clusters=2` to identify two primary customer segments. The `n_init='auto'` argument is used for more robust centroid initialization, and `random_state` ensures reproducibility. The cluster centroids are then transformed into the PCA space for plotting.\n",
        "\n",
        "**Explanation:** This code initializes and runs the K-Means algorithm. kmeans.fit_predict(X_pca) performs the clustering on the 2D PCA data and assigns a cluster label (0 or 1) to each data point, stored in kmeans_labels. pca.transform(kmeans.cluster_centers_) obtains the coordinates of the cluster centers in the same 2D PCA space, which are useful for plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "219861e0",
      "metadata": {
        "id": "219861e0"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans_labels = kmeans.fit_predict(X_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37484364",
      "metadata": {
        "id": "37484364"
      },
      "outputs": [],
      "source": [
        "centroids_pca = pca.fit_transform(kmeans.cluster_centers_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97d299c9",
      "metadata": {
        "id": "97d299c9"
      },
      "source": [
        "#### Visualization of K-Means Clusters\n",
        "\n",
        "This scatter plot visually represents the two clusters identified by K-Means in the 2D PCA space. Each data point is colored according to its assigned cluster, and the cluster centroids are explicitly marked by larger black circles.\n",
        "\n",
        "**Explanation:** This code generates a scatter plot. X_pca[:, 0] and X_pca[:, 1] provide the x and y coordinates from the PCA-reduced data. hue=kmeans_labels colors the points based on their cluster assignment. The plt.scatter call adds the black circle markers for the cluster centroids. The plot shows two broadly separated groups, indicating a high-level segmentation of the customer base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06883b39",
      "metadata": {
        "id": "06883b39"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "sns.scatterplot(x = X_pca[:, 0], y = X_pca[:, 1], hue = kmeans_labels, palette = 'viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='o', color='black', s=250)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd72ba9d",
      "metadata": {
        "id": "bd72ba9d"
      },
      "source": [
        "### Agglomerative Clustering\n",
        "\n",
        "Agglomerative Clustering is a hierarchical clustering method that builds a hierarchy of clusters by progressively merging smaller clusters into larger ones. We apply it here with `n_clusters=3` to explore if a more granular segmentation, with three distinct groups, is evident and provides better insights into customer behaviors.\n",
        "\n",
        "**Explanation:** This code initializes AgglomerativeClustering with a target of 3 clusters. agglo.fit_predict(X_pca) then performs the clustering on the 2D PCA data and assigns a cluster label (0, 1, or 2) to each data point, which is stored in agglo_labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28cdcd90",
      "metadata": {
        "id": "28cdcd90"
      },
      "outputs": [],
      "source": [
        "agglo = AgglomerativeClustering(n_clusters=3)\n",
        "agglo_labels = agglo.fit_predict(X_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d85c9cc",
      "metadata": {
        "id": "6d85c9cc"
      },
      "source": [
        "#### Visualization of Agglomerative Clusters\n",
        "\n",
        "This scatter plot displays the three customer segments identified by Agglomerative Clustering in the 2D PCA space. Points are colored according to their assigned cluster label to visually distinguish the groups.\n",
        "\n",
        "**Explanation:** This code generates another scatter plot. Similar to the K-Means plot, it uses X_pca for coordinates, but hue=agglo_labels colors the points based on the three clusters found by Agglomerative Clustering. This visualization reveals three distinct customer segments, which often appear more separated and interpretable than a two-cluster solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b633666",
      "metadata": {
        "id": "8b633666"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "sns.scatterplot(x = X_pca[:, 0], y = X_pca[:, 1], hue = agglo_labels, palette = 'viridis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9622c1c9",
      "metadata": {
        "id": "9622c1c9"
      },
      "source": [
        "### DBSCAN Clustering\n",
        "\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based algorithm that identifies clusters based on the density of data points. It groups together points that are closely packed, while marking isolated points as outliers (noise). Importantly, it does not require pre-specifying the number of clusters. Here, DBSCAN is applied to the *scaled* data (`X_Scaled`) rather than the PCA-reduced data, as density calculations are sensitive to the original feature space.\n",
        "\n",
        "**Explanation:** This code initializes DBSCAN with eps=1.5 (maximum distance between samples for them to be considered in the same neighborhood) and min_samples=5 (minimum number of samples in a neighborhood for a point to be considered a core point). db.fit_predict(X_Scaled) then performs the clustering on the scaled data and assigns cluster labels (including -1 for noise points) to db_labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3344a90d",
      "metadata": {
        "id": "3344a90d"
      },
      "outputs": [],
      "source": [
        "db = DBSCAN(eps=1.5, min_samples=5)\n",
        "\n",
        "db_labels = db.fit_predict(X_Scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "290c69e0",
      "metadata": {
        "id": "290c69e0"
      },
      "source": [
        "#### Visualization of DBSCAN Clusters\n",
        "\n",
        "This scatter plot visualizes the clusters identified by DBSCAN. Points are colored according to their assigned cluster label (`db_labels`), with points labeled `-1` representing noise or outliers. For practical visualization in 2D, the plot uses the first two features of the `X_Scaled` data, as `X_Scaled` is still high-dimensional.\n",
        "\n",
        "**Explanation:** This code generates a scatter plot showing the DBSCAN results. X_Scaled[:, 0] and X_Scaled[:, 1] are used for the x and y axes, displaying the relationship between the first two scaled features. hue=db_labels colors the points based on their assigned clusters or as noise. The plot highlights dense regions that form clusters and distinguishes them from sparser areas or isolated points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f575076d",
      "metadata": {
        "id": "f575076d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "sns.scatterplot(x = X_Scaled[:, 0], y = X_Scaled[:, 1], hue = db_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b224cc",
      "metadata": {
        "id": "d4b224cc"
      },
      "source": [
        "## 8. Hierarchical Clustering Visualization\n",
        "\n",
        "### Dendrogram\n",
        "\n",
        "To gain further insight into the hierarchical relationships within the customer data and potentially assist in determining a suitable number of clusters for hierarchical methods, a dendrogram is constructed. The `linkage` function calculates the linkages between data points using the `ward` method, which minimizes the variance within each cluster. The `dendrogram` function then visualizes this hierarchy, truncated to show the last 30 merged clusters.\n",
        "\n",
        "**Explanation:** sch.linkage(X_Scaled, method='ward') computes the hierarchical clustering linkages on the scaled data. sch.dendrogram then plots these linkages. truncate_mode = 'lastp' and p=30 mean that only the last 30 merge steps are shown, making the plot readable. The dendrogram visually represents how data points are grouped into clusters at various distances, with long vertical lines indicating significant distances where distinct clusters merge. This helps in identifying natural 'breaks' in the data that could suggest optimal cluster counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a35bca2",
      "metadata": {
        "id": "4a35bca2"
      },
      "outputs": [],
      "source": [
        "link = linkage(X_Scaled, method='ward')\n",
        "plt.figure(figsize=(15, 8))\n",
        "dendrogram(link,truncate_mode = 'lastp', p=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09586d20",
      "metadata": {
        "id": "09586d20"
      },
      "source": [
        "## Cluster Profiling (Example for K-Means)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8f280a",
      "metadata": {
        "id": "cd8f280a"
      },
      "outputs": [],
      "source": [
        "# First, add the cluster labels back to your original DataFrame (or data1)\n",
        "# Make sure 'data1' or 'df' has all the original features you want to profile.\n",
        "# Let's use 'data1' which has UserID removed, and fav_cuisine encoded.\n",
        "data_with_clusters = data1.copy()\n",
        "data_with_clusters['KMeans_Cluster'] = kmeans_labels # Add K-Means labels\n",
        "data_with_clusters['Agglo_Cluster'] = agglo_labels   # Add Agglomerative labels\n",
        "data_with_clusters['DBSCAN_Cluster'] = db_labels     # Add DBSCAN labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7188d572",
      "metadata": {
        "id": "7188d572"
      },
      "outputs": [],
      "source": [
        "# --- Profile K-Means Clusters ---\n",
        "print(\"--- K-Means Cluster Profiles (Mean Values) ---\")\n",
        "kmeans_profile = data_with_clusters.groupby('KMeans_Cluster')[X.columns].mean() # Use X.columns for features\n",
        "print(kmeans_profile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b4aac4e",
      "metadata": {
        "id": "9b4aac4e"
      },
      "outputs": [],
      "source": [
        "# You can also add standard deviations or other aggregates if helpful\n",
        "# kmeans_std = data_with_clusters.groupby('KMeans_Cluster')[X.columns].std()\n",
        "# print(\"\\n--- K-Means Cluster Profiles (Standard Deviation) ---\")\n",
        "# print(kmeans_std)\n",
        "\n",
        "# --- Profile Agglomerative Clusters ---\n",
        "print(\"\\n--- Agglomerative Cluster Profiles (Mean Values) ---\")\n",
        "agglo_profile = data_with_clusters.groupby('Agglo_Cluster')[X.columns].mean()\n",
        "print(agglo_profile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c563a596",
      "metadata": {
        "id": "c563a596"
      },
      "outputs": [],
      "source": [
        "# --- Profile DBSCAN Clusters ---\n",
        "# Remember DBSCAN can have a -1 cluster for noise\n",
        "print(\"\\n--- DBSCAN Cluster Profiles (Mean Values) ---\")\n",
        "dbscan_profile = data_with_clusters.groupby('DBSCAN_Cluster')[X.columns].mean()\n",
        "print(dbscan_profile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b017447",
      "metadata": {
        "id": "4b017447"
      },
      "outputs": [],
      "source": [
        "# Visualize profiles using bar plots\n",
        "# Example for K-Means:\n",
        "kmeans_profile.T.plot(kind='bar', figsize=(12, 7))\n",
        "plt.title('K-Means Cluster Feature Means')\n",
        "plt.ylabel('Mean Value')\n",
        "plt.xlabel('Feature')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Cluster')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Do similar plots for Agglomerative and DBSCAN profiles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27b38cfb",
      "metadata": {
        "id": "27b38cfb"
      },
      "source": [
        "## 9. Key Insights and Observations\n",
        "\n",
        "Based on our comprehensive data exploration and application of various clustering algorithms, several key insights have been observed:\n",
        "\n",
        "*   **Data Quality:** The dataset is clean, with no missing values or duplicates, ensuring reliable analysis.\n",
        "*   **Customer Diversity:** The descriptive statistics highlight a broad range in customer demographics (age) and behavioral attributes (total orders, average spend, app usage time), indicating a heterogeneous customer base.\n",
        "*   **Cuisine Preference Distribution:** The analysis of `fav_cuisine` shows an uneven distribution, suggesting certain cuisine types are significantly more popular than others.\n",
        "*   **Clustering Performance:**\n",
        "    *   K-Means (2 clusters) provides a high-level segmentation, broadly separating customers.\n",
        "    *   Agglomerative Clustering (3 clusters) appears to offer a more nuanced and visually distinct segmentation, potentially revealing more actionable customer groups.\n",
        "    *   DBSCAN identified density-based clusters and isolated outliers, which is useful for pinpointing core customer concentrations and unique profiles.\n",
        "*   **Hierarchical Structure:** The dendrogram supports the existence of natural groupings within the data, which can guide decisions on the optimal number of clusters for hierarchical approaches.\n",
        "\n",
        "These insights are foundational for developing targeted business strategies, such as customized marketing campaigns, personalized recommendations, or improvements in service delivery tailored to specific customer segments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3708b588",
      "metadata": {
        "id": "3708b588"
      },
      "source": [
        "## 10. Conclusion\n",
        "\n",
        "This project successfully performed a detailed customer segmentation analysis on the food delivery dataset. We initiated with thorough data loading and exploration, followed by essential data preprocessing steps including feature encoding, scaling, and dimensionality reduction. Subsequently, multiple clustering algorithms (K-Means, Agglomerative Clustering, and DBSCAN) were applied, and their results were visualized. The hierarchical structure of the data was also explored through a dendrogram.\n",
        "\n",
        "The application of various clustering techniques provided different perspectives on customer grouping, with Agglomerative Clustering showing promising visual separation into three segments.\n",
        "\n",
        "The identified customer segments represent distinct groups based on their behavioral and demographic attributes. These segments are valuable for strategic decision-making.\n",
        "\n",
        "**Next steps could include:**\n",
        "\n",
        "*   **Cluster Profiling:** Conduct a detailed analysis of the characteristics (e.g., mean values of original features) for each identified cluster to understand the unique profile of each customer segment.\n",
        "*   **Optimal Cluster Number Determination:** Employ quantitative evaluation metrics (e.g., Silhouette Score, Elbow Method) to formally determine the most suitable number of clusters for each algorithm.\n",
        "*   **Actionable Business Recommendations:** Translate the insights from the customer segments into concrete, data-driven business strategies, such as targeted promotions, personalized service offerings, or tailored loyalty programs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e20760",
      "metadata": {
        "id": "04e20760"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}