{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f2dcc279",
      "metadata": {
        "id": "f2dcc279"
      },
      "source": [
        "# Credit Card Fraud Detection\n",
        "\n",
        "In this notebook, we construct and assess a model that can flag suspicious transactions as potentially fraudulent using credit card transaction data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2a8c13",
      "metadata": {
        "id": "fd2a8c13"
      },
      "source": [
        "### Step 1: Environment and Data Initialization\n",
        "\n",
        "We begin by bringing in essential libraries that support data operations, analysis, and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33cd9291",
      "metadata": {
        "id": "33cd9291"
      },
      "outputs": [],
      "source": [
        "import numpy as numpy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "584d2fac",
      "metadata": {
        "id": "584d2fac"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"C:/Users/DELL/Downloads/creditcard (1).csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f4da7a",
      "metadata": {
        "id": "94f4da7a"
      },
      "source": [
        "# Step 2: Exploratory Data Analysis (EDA)\n",
        "\n",
        "Understanding the dataset's dimensions and contents helps us prepare for model development by identifying potential issues and opportunities in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f593be5",
      "metadata": {
        "id": "9f593be5"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bd6becd",
      "metadata": {
        "id": "9bd6becd"
      },
      "source": [
        "This line reveals the structure of the dataset, highlighting the total number of records and features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "255ef563",
      "metadata": {
        "id": "255ef563"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "814fa5da",
      "metadata": {
        "id": "814fa5da"
      },
      "source": [
        "```df.isna().sum()```  \n",
        "This checks for missing (NaN/null) values in each column.  \n",
        "There are no missing values in any of the columns. This is excellent, as it means we don't need to perform any imputation or removal of rows/columns due to missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27a934fc",
      "metadata": {
        "id": "27a934fc"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07cb3c36",
      "metadata": {
        "id": "07cb3c36"
      },
      "source": [
        "This provides descriptive statistics for numerical columns, such as count, mean, standard deviation, min, max, and quartile values.  \n",
        "Most columns (V1 through V28) are anonymized features, likely obtained through a technique called PCA (Principal Component Analysis) to protect user privacy.  \n",
        "The Time column represents the seconds elapsed between each transaction and the first transaction in the dataset.  \n",
        "The Amount column shows the transaction amount.  \n",
        "The Class column is our target variable, where 0 indicates a legitimate transaction and 1 indicates a fraudulent one.  \n",
        "The mean of the Class column is 0.001727, which is very close to zero.  \n",
        "This immediately signals a severe class imbalance: fraudulent transactions are very rare."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c8c5746",
      "metadata": {
        "id": "4c8c5746"
      },
      "source": [
        "#### Renaming 'Class' Values\n",
        "The numerical 0 and 1 in the Class column are replaced with more descriptive labels 'Not Fraud' and 'Fraud' for better readability and understanding of results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df1d15dd",
      "metadata": {
        "id": "df1d15dd"
      },
      "outputs": [],
      "source": [
        "df['Class'] = df['Class'].replace({0:'Not Fraud',1:'Fraud'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31b5ed4d",
      "metadata": {
        "id": "31b5ed4d"
      },
      "source": [
        "This counts the occurrences of each unique value in the 'Class' column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c2ba74a",
      "metadata": {
        "id": "3c2ba74a"
      },
      "outputs": [],
      "source": [
        "df['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c00f1f0",
      "metadata": {
        "id": "4c00f1f0"
      },
      "source": [
        "This confirms the extreme imbalance in the dataset. Out of 284,807 transactions, 284,315 are legitimate, while only 492 are fraudulent. This means fraud accounts for approximately (492 / 284807) * 100 = 0.17% of the total transactions. This imbalance is a significant challenge for machine learning models, as they might be biased towards the majority class ('Not Fraud')."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59ff91d8",
      "metadata": {
        "id": "59ff91d8"
      },
      "source": [
        "#### Fraud Pie Chart\n",
        "A pie chart is generated to visually represent the distribution of 'Fraud' vs. 'Not Fraud' transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c08605",
      "metadata": {
        "id": "a0c08605"
      },
      "outputs": [],
      "source": [
        "fraud = df['Class'].value_counts()\n",
        "label = ['Not Fraud', 'Fraud']\n",
        "color = ['green','red']\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.pie(fraud, labels=label, autopct='%1.1f%%' )\n",
        "plt.title(\"Fraud Pie Chart\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cdad745",
      "metadata": {
        "id": "5cdad745"
      },
      "source": [
        " The pie chart dramatically shows the imbalance, with the 'Fraud' slice being almost zero, highlighting the rarity of fraudulent transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c61050a",
      "metadata": {
        "id": "7c61050a"
      },
      "source": [
        "# 3. Data Preparation\n",
        "Before training a machine learning model, the data needs to be structured appropriately."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88029e03",
      "metadata": {
        "id": "88029e03"
      },
      "source": [
        "#### Separating Features (X) and Target (Y)  \n",
        "The dataset is divided into two parts:  \n",
        "x (Features): Contains all columns from the DataFrame except the 'Class' column. These are the input variables the model will use to make predictions.  \n",
        "y (Target): Contains only the 'Class' column. This is the output variable that the model will try to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c654a35",
      "metadata": {
        "id": "9c654a35"
      },
      "outputs": [],
      "source": [
        "x = df.drop('Class',axis=1)\n",
        "y = df['Class']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ab38244",
      "metadata": {
        "id": "0ab38244"
      },
      "source": [
        "#### Splitting Data into Training and Testing Sets  \n",
        "The dataset is split into training and testing subsets to evaluate the model's performance on unseen data.  \n",
        "train_test_split: A function from scikit-learn for splitting arrays or matrices into random train and test subsets.  \n",
        "test_size=0.2: 20% of the data will be used for testing, and the remaining 80% for training.  \n",
        "random_state=7: This ensures reproducibility. If you run the code multiple times, the data will be split in the exact same way.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e4ed392",
      "metadata": {
        "id": "2e4ed392"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf1b061",
      "metadata": {
        "id": "3cf1b061"
      },
      "source": [
        "Output: len(x_train),len(x_test) shows (227845, 56962).  \n",
        "This confirms that 227,845 samples are used for training and 56,962 samples for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb4836d1",
      "metadata": {
        "id": "cb4836d1"
      },
      "source": [
        "#### Feature Scaling (StandardScaler)\n",
        "Feature scaling is applied to standardize the range of independent variables or features.\n",
        "\n",
        "StandardScaler: Transforms data such that its mean is 0 and standard deviation is 1 (Z-score normalization). This is important for algorithms that are sensitive to feature scales (like Logistic Regression or K-Nearest Neighbors).  \n",
        "\n",
        "sc.fit_transform(x_train): The StandardScaler is fitted (learns the mean and standard deviation) only on the training data and then transforms it.  \n",
        "\n",
        "sc.transform(x_test): The same scaler (with means and standard deviations learned from x_train) is used to transform the test data. This prevents \"data leakage\" where information from the test set could influence the training process.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcab14c7",
      "metadata": {
        "id": "bcab14c7"
      },
      "outputs": [],
      "source": [
        "len(x_train),len(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2460ffe3",
      "metadata": {
        "id": "2460ffe3"
      },
      "source": [
        "# 4. Model Training & Evaluation\n",
        "This section involves selecting a machine learning model, training it, and then evaluating its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ea2eb6",
      "metadata": {
        "id": "46ea2eb6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4651698d",
      "metadata": {
        "id": "4651698d"
      },
      "outputs": [],
      "source": [
        "sc = StandardScaler()\n",
        "\n",
        "x_train_sc = sc.fit_transform(x_train)\n",
        "x_test_sc = sc.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "305c4c4e",
      "metadata": {
        "id": "305c4c4e"
      },
      "source": [
        "#### Logistic Regression Model\n",
        "Logistic Regression is a common algorithm for binary classification problems.  \n",
        "\n",
        "LogisticRegression(): An instance of the Logistic Regression model is created.  \n",
        "model.fit(x_train_sc, y_train): The model is trained using the scaled training features (x_train_sc) and their corresponding target labels (y_train).  \n",
        "y_pred = model.predict(x_test_sc): After training, the model predicts the 'Class' for the scaled test features (x_test_sc).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a39725",
      "metadata": {
        "id": "41a39725"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d635079",
      "metadata": {
        "id": "6d635079"
      },
      "outputs": [],
      "source": [
        "lr_model = LogisticRegression()\n",
        "\n",
        "lr_model.fit(x_train_sc,y_train)\n",
        "y_pred  =lr_model.predict(x_test_sc)\n",
        "\n",
        "lr_accu = accuracy_score(y_pred,y_test)\n",
        "print(\"Logistic regression accuracy_score: \",lr_accu*100)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "\n",
        "import seaborn as sns\n",
        "cm_lr=confusion_matrix(y_test,y_pred)\n",
        "\n",
        "sns.heatmap(cm_lr,annot=True,cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e52db9a",
      "metadata": {
        "id": "8e52db9a"
      },
      "outputs": [],
      "source": [
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Fraud', 'Fraud']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c6f639c",
      "metadata": {
        "id": "0c6f639c"
      },
      "source": [
        "#### Accuracy Score\n",
        "Accuracy measures the proportion of total predictions that were correct.  \n",
        "\n",
        "lr_accu = accuracy_score(y_pred,y_test): Calculates the accuracy by comparing the model's predictions (y_pred) with the actual test labels (y_test).  \n",
        "Output: accuracy_score: 99.90344440153085  \n",
        "Explanation: The model achieved an accuracy of approximately 99.90%. While this number seems very high, it is misleading in the context of our highly imbalanced dataset.  \n",
        "\n",
        "#### A simple model that always predicts 'Not Fraud' would achieve an accuracy of 284315 / 284807 ≈ 99.90%. Therefore, accuracy alone is not a sufficient metric for evaluating fraud detection models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5faa216c",
      "metadata": {
        "id": "5faa216c"
      },
      "source": [
        "#### Confusion Matrix\n",
        "\n",
        "A confusion matrix provides a more detailed breakdown of correct and incorrect classifications for each class.  \n",
        "cm = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted']): This command generates the confusion matrix table.  \n",
        "#### Interpretation of the Confusion Matrix:\n",
        "True Negatives (TN - Not Fraud Actual, Not Fraud Predicted): 56,851 transactions were correctly identified as legitimate.  \n",
        "False Positives (FP - Not Fraud Actual, Fraud Predicted): 11 legitimate transactions were incorrectly classified as fraudulent (false alarms).  \n",
        "False Negatives (FN - Fraud Actual, Not Fraud Predicted): 44 fraudulent transactions were incorrectly classified as legitimate. This is the most critical error in fraud detection, as these are the frauds that go undetected.  \n",
        "True Positives (TP - Fraud Actual, Fraud Predicted): 56 fraudulent transactions were correctly identified.  \n",
        "Conclusion from Confusion Matrix: Out of a total of 56 + 44 = 100 actual fraudulent transactions in the test set, the model only successfully detected 56 of them. This means 44% of the actual fraud cases were missed by the model, which is a significant weakness for a fraud detection system, despite the high overall accuracy.\n",
        "\n",
        "#### Confusion Matrix Heatmap\n",
        "The confusion matrix is visualized as a heatmap for easier interpretation.\n",
        "The heatmap clearly shows the vast number of correctly predicted 'Not Fraud' transactions (the large value in the bottom-right cell) and the smaller but significant number of missed frauds (False Negatives) in the top-right cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e99e4c5",
      "metadata": {
        "id": "3e99e4c5"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(x_train_sc, y_train)\n",
        "y_pred_dt = dt_model.predict(x_test_sc)\n",
        "\n",
        "dt_accu = accuracy_score(y_pred_dt, y_test)\n",
        "\n",
        "print(\"Decision Tree Accuracy: \",dt_accu * 100)\n",
        "\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_dt)\n",
        "\n",
        "sns.heatmap(cm_dt,annot=True,cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba0b3277",
      "metadata": {
        "id": "ba0b3277"
      },
      "source": [
        "## Decision Tree Model\n",
        "Decision Trees classify data by learning simple decision rules inferred from features, forming a tree-like structure.\n",
        "\n",
        "Explanation: A DecisionTreeClassifier is initialized and trained, then used to predict on the test set.\n",
        "\n",
        "Performance Metrics\n",
        "Accuracy Score:\n",
        "Output: Decision Tree Accuracy: [e.g., 99.91]%\n",
        "Interpretation: Similar to Logistic Regression, overall accuracy can be deceptive; deeper analysis is required.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0dbc5b8",
      "metadata": {
        "id": "a0dbc5b8"
      },
      "outputs": [],
      "source": [
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_dt, target_names=['Not Fraud', 'Fraud']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d0ad4d2",
      "metadata": {
        "id": "1d0ad4d2"
      },
      "source": [
        "Interpretation: Highlights the precision, recall, and F1-score, with recall for 'Fraud' being a key indicator of fraud detection capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ca9e3df",
      "metadata": {
        "id": "2ca9e3df"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "rf_model.fit(x_train_sc, y_train)\n",
        "\n",
        "y_pred_rf = rf_model.predict(x_test_sc)\n",
        "\n",
        "rf_accu = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy: \",rf_accu * 100)\n",
        "\n",
        "cm_rf = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_rf)\n",
        "\n",
        "sns.heatmap(cm_rf,annot=True,cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e89f99",
      "metadata": {
        "id": "29e89f99"
      },
      "source": [
        "## Random Forest Model\n",
        "Random Forest is an ensemble method that builds multiple decision trees and merges their predictions to improve accuracy and control overfitting.\n",
        "\n",
        "Explanation: A RandomForestClassifier is trained on the scaled data, and predictions are made.\n",
        "\n",
        "Performance Metrics\n",
        "Accuracy Score:\n",
        "Output: Random Forest Accuracy: [e.g., 99.95]%\n",
        "Interpretation: Often higher than single trees, but still requires scrutiny beyond raw accuracy for imbalanced data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "675a40ed",
      "metadata": {
        "id": "675a40ed"
      },
      "outputs": [],
      "source": [
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=['Not Fraud', 'Fraud']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a69c8dfd",
      "metadata": {
        "id": "a69c8dfd"
      },
      "source": [
        "Interpretation: Generally shows robust performance across metrics due to its ensemble nature, making it suitable for fraud detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b876bf8",
      "metadata": {
        "id": "1b876bf8"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_model = KNeighborsClassifier()\n",
        "\n",
        "knn_model.fit(x_train_sc, y_train)\n",
        "\n",
        "y_pred_knn = knn_model.predict(x_test_sc)\n",
        "\n",
        "knn_accu = accuracy_score(y_test, y_pred_knn)\n",
        "print(\"K-Nearest Neighbors Accuracy: \",knn_accu * 100)\n",
        "\n",
        "cm_knn = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_knn)\n",
        "\n",
        "sns.heatmap(cm_knn,annot=True,cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "694ce436",
      "metadata": {
        "id": "694ce436"
      },
      "source": [
        "## K-Nearest Neighbors (KNN) Model\n",
        "KNN is a non-parametric, instance-based learning algorithm that classifies new data points based on the majority class of their 'k' nearest neighbors.\n",
        "\n",
        "Explanation: A KNeighborsClassifier is trained, and predictions are generated for the test set.\n",
        "\n",
        "Performance Metrics\n",
        "Accuracy Score:\n",
        "Output: K-Nearest Neighbors Accuracy: [e.g., 99.95]%\n",
        "Interpretation: KNN's performance is sensitive to k and feature scaling; accuracy alone is insufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07734d1d",
      "metadata": {
        "id": "07734d1d"
      },
      "outputs": [],
      "source": [
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_knn, target_names=['Not Fraud', 'Fraud']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03e98df",
      "metadata": {
        "id": "a03e98df"
      },
      "source": [
        "Interpretation: Crucial for understanding the balance between precision and recall for the minority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ad0f6cc",
      "metadata": {
        "id": "7ad0f6cc"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "svm_model.fit(x_train_sc, y_train)\n",
        "y_pred_svm = svm_model.predict(x_test_sc)\n",
        "\n",
        "svc_accu = accuracy_score(y_test, y_pred_svm)\n",
        "print(\"SVM Accuracy: \", svc_accu * 100)\n",
        "\n",
        "cm_svc = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_svc)\n",
        "\n",
        "sns.heatmap(cm_svc,annot=True,cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5caff829",
      "metadata": {
        "id": "5caff829"
      },
      "source": [
        "## Support Vector Machine (SVM) Model\n",
        "SVMs are powerful supervised learning models that find an optimal hyperplane to separate data points into classes, maximizing the margin between them.\n",
        "\n",
        "Explanation: An SVC (Support Vector Classifier) is trained and used to make predictions.\n",
        "\n",
        "Performance Metrics\n",
        "Accuracy Score:\n",
        "Output: SVM Accuracy: [e.g., 99.94]%\n",
        "Interpretation: SVMs can achieve high accuracy but are computationally intensive for large datasets; raw accuracy needs careful interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de693120",
      "metadata": {
        "id": "de693120"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm, target_names=['Not Fraud', 'Fraud']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982fef0c",
      "metadata": {
        "id": "982fef0c"
      },
      "source": [
        "Interpretation: Evaluating precision and recall for the 'Fraud' class is key to assessing its practical utility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6f79af",
      "metadata": {
        "id": "da6f79af"
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "lda_model = LinearDiscriminantAnalysis()\n",
        "lda_model.fit(x_train_sc, y_train)\n",
        "y_pred_lda = lda_model.predict(x_test_sc)\n",
        "\n",
        "lda_accu = accuracy_score(y_test, y_pred_lda)\n",
        "print(\"LDA Accuracy: \", lda_accu * 100)\n",
        "\n",
        "print(\"LDA Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lda))\n",
        "\n",
        "cm_lda = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_lda)\n",
        "\n",
        "sns.heatmap(cm_lda,annot=True,cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8f5131e",
      "metadata": {
        "id": "c8f5131e"
      },
      "source": [
        "## Linear Discriminant Analysis (LDA) Model\n",
        "LDA is a classification and dimensionality reduction technique that projects data onto a lower-dimensional space to maximize class separability.\n",
        "\n",
        "Explanation: A LinearDiscriminantAnalysis model is trained and used for predictions.\n",
        "\n",
        "Performance Metrics\n",
        "Accuracy Score:\n",
        "Output: LDA Accuracy: [e.g., 99.95]%\n",
        "Interpretation: LDA assumes normal distribution and equal covariance; recall for fraud cases is critical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8ed39db",
      "metadata": {
        "id": "e8ed39db"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_lda, target_names=['Not Fraud', 'Fraud']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b9801c",
      "metadata": {
        "id": "45b9801c"
      },
      "source": [
        "Interpretation: Helps assess LDA's ability to distinguish classes based on its linear decision boundary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8c3763e",
      "metadata": {
        "id": "a8c3763e"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb_model = GaussianNB()\n",
        "gnb_model.fit(x_train_sc, y_train)\n",
        "y_pred_gnb = gnb_model.predict(x_test_sc)\n",
        "\n",
        "gnb_accu = accuracy_score(y_test, y_pred_gnb)\n",
        "print(\"Gaussian Naive Bayes Accuracy: \", gnb_accu * 100)\n",
        "\n",
        "print(\"Gaussian Naive Bayes Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gnb))\n",
        "\n",
        "cm_gnb = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_gnb)\n",
        "\n",
        "sns.heatmap(cm_gnb,annot=True,cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef205bb",
      "metadata": {
        "id": "6ef205bb"
      },
      "source": [
        "## Gaussian Naive Bayes Model\n",
        "Gaussian Naive Bayes is a probabilistic classifier based on Bayes' theorem, assuming feature independence and Gaussian distribution.\n",
        "\n",
        "Explanation: A GaussianNB model is trained and used to generate predictions.\n",
        "\n",
        "Performance Metrics\n",
        "Accuracy Score:\n",
        "Output: Gaussian Naive Bayes Accuracy: [e.g., 97.72]%\n",
        "Interpretation: Computationally efficient, but its strong independence assumption can limit performance on complex data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ba31a8",
      "metadata": {
        "id": "95ba31a8"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_gnb, target_names=['Not Fraud', 'Fraud']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a70fb90",
      "metadata": {
        "id": "0a70fb90"
      },
      "source": [
        "Interpretation: Helps assess the balance between identifying true frauds (recall) and minimizing false alarms (precision)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8151be4",
      "metadata": {
        "id": "d8151be4"
      },
      "source": [
        "# 5. Conclusion\n",
        "\n",
        "This project shows just how challenging it is to detect fraud when real fraud cases are so rare in the data. Even though models like Random Forest reached over 99% accuracy, that number can be misleading. The confusion matrix and detailed metrics revealed that many fraudulent transactions still slipped through undetected — which is a big problem in banking.\n",
        "\n",
        "In fraud detection, catching as many real frauds as possible is more important than just having high overall accuracy. That’s why improving the recall for fraud cases should be the top priority. Going forward, it’s clear we need to move beyond basic models and try smarter solutions — like balancing the dataset with techniques such as SMOTE, trying better ensemble methods, or even cost-sensitive learning.\n",
        "\n",
        "By combining these advanced strategies, we can build a more reliable fraud detection system that actually protects customers and companies from hidden threats.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}