{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1be98e1",
   "metadata": {},
   "source": [
    "# Walmart ML + ANN Assignment Template\n",
    "\n",
    "**Course:** Neural Networks / Machine Learning  \n",
    "**Dataset:** `Walmart.csv`  \n",
    "**Project Type:** Regression (default)\n",
    "\n",
    "> This notebook is structured to satisfy all mandatory assignment requirements. Run cells in order and replace placeholder interpretations with your final observations where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839fffcf",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Executive Summary & Problem Understanding\n",
    "\n",
    "### Dataset Description\n",
    "The Walmart dataset contains store-level weekly sales and business context variables such as holiday flag, temperature, fuel price, CPI, unemployment, and engineered date-based fields.\n",
    "\n",
    "### Problem Statement\n",
    "Build a predictive model to estimate weekly sales accurately and identify the strongest business drivers.\n",
    "\n",
    "### Objective\n",
    "- Perform robust cleaning and preprocessing\n",
    "- Derive insights via EDA\n",
    "- Build and compare multiple ML models\n",
    "- Optimize best model and compare with ANN\n",
    "- Provide business-ready interpretation and recommendations\n",
    "\n",
    "### Key Findings (from executed outputs)\n",
    "- EDA Insight 1: `Store` has the strongest absolute correlation with `Weekly_Sales` (about -0.337), indicating strong store-level heterogeneity.\n",
    "- EDA Insight 2: `Unemployment`, `CPI`, and `Temperature` show weaker but meaningful negative relationships with weekly sales.\n",
    "- ML Insight 1: Among baseline ML models, `RandomForest` performed best (RMSE ‚âà 132,044; R¬≤ ‚âà 0.944).\n",
    "- ML Insight 2: Tuning improved RF slightly (RMSE ‚âà 131,703; MAE ‚âà 72,068; R¬≤ ‚âà 0.9445), while ANN underperformed the tuned RF.\n",
    "\n",
    "### Final Recommendation\n",
    "Use **Random Forest (After Tuning)** as the production recommendation because it achieved the best overall test performance and clear robustness versus alternative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "TENSORFLOW_AVAILABLE = True\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "    from tensorflow.keras.optimizers import Adam, SGD\n",
    "except Exception as e:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print('TensorFlow not available. ANN section will be skipped unless TensorFlow is installed.')\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7b8db",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Data Cleaning & Preprocessing\n",
    "\n",
    "This section documents and justifies each preprocessing choice:\n",
    "- dataset structure\n",
    "- missing values and treatment strategy\n",
    "- duplicate handling\n",
    "- outlier treatment\n",
    "- encoding and scaling\n",
    "- final cleaned dataset summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c5f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_PATH = 'Walmart.csv'\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "df = df_raw.copy()\n",
    "\n",
    "print('Initial Shape:', df.shape)\n",
    "display(df.head())\n",
    "display(df.dtypes)\n",
    "\n",
    "# Parse date if present\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype('Int64')\n",
    "\n",
    "# Dataset structure summary\n",
    "structure_df = pd.DataFrame({\n",
    "    'column': df.columns,\n",
    "    'dtype': df.dtypes.astype(str).values,\n",
    "    'missing_count': df.isna().sum().values,\n",
    "    'missing_percent': (df.isna().mean() * 100).round(2).values,\n",
    "    'n_unique': df.nunique(dropna=False).values\n",
    "}).sort_values('missing_percent', ascending=False)\n",
    "\n",
    "print('\\nDataset structure summary:')\n",
    "display(structure_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3943e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values and duplicates\n",
    "missing_df = (df.isna().mean() * 100).sort_values(ascending=False).rename('missing_%').to_frame()\n",
    "print('Missing value percentage by column:')\n",
    "display(missing_df)\n",
    "\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f'Duplicate rows before treatment: {duplicate_count}')\n",
    "if duplicate_count > 0:\n",
    "    df = df.drop_duplicates().copy()\n",
    "print(f'Shape after duplicate handling: {df.shape}')\n",
    "\n",
    "# Outlier treatment: IQR capping for numeric features (excluding target later)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    q1, q3 = df[col].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    df[col] = df[col].clip(lower, upper)\n",
    "\n",
    "print('Applied IQR-based capping on numeric columns to reduce extreme outlier impact.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa5d30",
   "metadata": {},
   "source": [
    "### Preprocessing Justification\n",
    "\n",
    "- **Missing values:** Median imputation for numeric and most-frequent imputation for categorical columns are robust and preserve sample size.\n",
    "- **Duplicates:** Exact duplicate rows are removed to avoid biased learning and duplicated signal.\n",
    "- **Outliers:** IQR capping is used instead of deletion to retain records while reducing distortion from extreme values.\n",
    "- **Encoding:** One-hot encoding is applied to categorical features to make them model-compatible.\n",
    "- **Scaling:** Standardization is applied for scale-sensitive models (Linear Regression, ANN, SVM-like methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d2cd1",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Exploratory Data Analysis (EDA)\n",
    "\n",
    "### üîπ Univariate Analysis\n",
    "- Distribution plots\n",
    "- Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe1fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate analysis\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print('Summary statistics (numeric):')\n",
    "display(df[num_cols].describe().T)\n",
    "\n",
    "plot_cols = num_cols[:6] if len(num_cols) >= 6 else num_cols\n",
    "if plot_cols:\n",
    "    n = len(plot_cols)\n",
    "    fig, axes = plt.subplots((n + 1)//2, 2, figsize=(14, 4*((n + 1)//2)))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for i, col in enumerate(plot_cols):\n",
    "        sns.histplot(df[col], kde=True, ax=axes[i], color='steelblue')\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25608fa6",
   "metadata": {},
   "source": [
    "**Univariate Interpretation (2‚Äì3 lines):**\n",
    "- `Weekly_Sales` has a wide spread (roughly from 0.21M to 2.72M), indicating substantial variation across stores/time.\n",
    "- `Temperature`, `Fuel_Price`, `CPI`, and `Unemployment` appear within realistic business ranges after IQR capping.\n",
    "- `Holiday_Flag` shows no variation in the processed data (mostly/entirely 0), so its direct predictive contribution is limited here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384aed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis: feature vs target and correlations\n",
    "TARGET_CANDIDATES = ['Weekly_Sales', 'Sales', 'Target', 'y']\n",
    "target_col = next((c for c in TARGET_CANDIDATES if c in df.columns), None)\n",
    "if target_col is None:\n",
    "    target_col = df.select_dtypes(include=[np.number]).columns[-1]\n",
    "\n",
    "print(f'Selected target column: {target_col}')\n",
    "\n",
    "corr = df.select_dtypes(include=[np.number]).corr(numeric_only=True)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top numeric relationships with target\n",
    "if target_col in corr.columns:\n",
    "    top_predictors = corr[target_col].drop(target_col).abs().sort_values(ascending=False).head(4).index.tolist()\n",
    "    for col in top_predictors:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.scatterplot(x=df[col], y=df[target_col], alpha=0.6)\n",
    "        plt.title(f'{col} vs {target_col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(target_col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Category comparisons\n",
    "for c in [col for col in cat_cols if col != 'Date'][:2]:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=df[c], y=df[target_col])\n",
    "    plt.title(f'{c} vs {target_col}')\n",
    "    plt.xlabel(c)\n",
    "    plt.ylabel(target_col)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16078387",
   "metadata": {},
   "source": [
    "**Bivariate Interpretation (2‚Äì3 lines):**\n",
    "- `Store` and `Unemployment` show the strongest visible relationship with `Weekly_Sales` among numeric variables.\n",
    "- Correlation signs suggest that increases in `Unemployment` tend to reduce weekly sales in this dataset.\n",
    "- Category-level comparisons indicate that store-level segmentation is more informative than holiday segmentation for this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5175d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate analysis and interaction trends\n",
    "if {'Store', 'Month', target_col}.issubset(df.columns):\n",
    "    pivot_data = df.pivot_table(index='Store', columns='Month', values=target_col, aggfunc='mean')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(pivot_data, cmap='YlGnBu')\n",
    "    plt.title(f'Store-Month Interaction Heatmap ({target_col})')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Store')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 'Holiday_Flag' in df.columns and 'Month' in df.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.lineplot(data=df.groupby(['Month', 'Holiday_Flag'])[target_col].mean().reset_index(), x='Month', y=target_col, hue='Holiday_Flag', marker='o')\n",
    "    plt.title(f'Monthly {target_col} by Holiday Flag')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Strong predictor identification (numeric)\n",
    "if target_col in corr.columns:\n",
    "    strong_preds = corr[target_col].drop(target_col).sort_values(key=np.abs, ascending=False)\n",
    "    print('Strong predictor ranking (numeric):')\n",
    "    display(strong_preds.to_frame('corr_with_target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e516ba3",
   "metadata": {},
   "source": [
    "**Multivariate Interpretation (2‚Äì3 lines):**\n",
    "- Interactions between `Store` and `Month` indicate that sales levels differ much more by store than by monthly seasonality alone.\n",
    "- Segment patterns suggest high-store heterogeneity, which supports models that capture non-linear interactions.\n",
    "- These trends justify engineered interaction features and tree-based methods (especially Random Forest) as strong modeling choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1765b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-generate minimum 5 meaningful insights (draft)\n",
    "insights = []\n",
    "\n",
    "if target_col in corr.columns:\n",
    "    ranked = corr[target_col].drop(target_col).sort_values(key=np.abs, ascending=False)\n",
    "    for feature, val in ranked.head(3).items():\n",
    "        direction = 'positive' if val > 0 else 'negative'\n",
    "        insights.append(f'{feature} has a {direction} correlation ({val:.3f}) with {target_col}.')\n",
    "\n",
    "if 'Holiday_Flag' in df.columns:\n",
    "    holiday_means = df.groupby('Holiday_Flag')[target_col].mean()\n",
    "    if len(holiday_means) >= 2:\n",
    "        diff = holiday_means.max() - holiday_means.min()\n",
    "        insights.append(f'Holiday vs non-holiday average {target_col} differs by about {diff:,.2f}.')\n",
    "\n",
    "if 'Month' in df.columns:\n",
    "    month_means = df.groupby('Month')[target_col].mean()\n",
    "    peak_m, low_m = month_means.idxmax(), month_means.idxmin()\n",
    "    insights.append(f'Peak month is {peak_m} and lowest month is {low_m} based on mean {target_col}.')\n",
    "\n",
    "print('Minimum 5 meaningful insights (draft):')\n",
    "for i, txt in enumerate(insights[:5], 1):\n",
    "    print(f'{i}. {txt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a26f9",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Feature Engineering, Selection & Model Development\n",
    "\n",
    "This section includes:\n",
    "- At least 2 engineered features\n",
    "- Feature selection (correlation + RFE)\n",
    "- Model training and comparison table (Linear, Decision Tree, Random Forest, Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering (minimum 2)\n",
    "if {'Fuel_Price', 'CPI'}.issubset(df.columns):\n",
    "    df['Fuel_CPI_Ratio'] = df['Fuel_Price'] / (df['CPI'] + 1e-6)\n",
    "\n",
    "if {'Unemployment', 'Temperature'}.issubset(df.columns):\n",
    "    df['Unemp_Temp_Interaction'] = df['Unemployment'] * df['Temperature']\n",
    "\n",
    "if {'Holiday_Flag', 'WeekOfYear'}.issubset(df.columns):\n",
    "    df['Holiday_Week_Interaction'] = df['Holiday_Flag'] * df['WeekOfYear'].fillna(0)\n",
    "\n",
    "# Prepare X, y\n",
    "X = df.drop(columns=[target_col]).copy()\n",
    "y = df[target_col]\n",
    "\n",
    "# Remove raw datetime columns (SimpleImputer does not support datetime dtype)\n",
    "datetime_cols = X.select_dtypes(include=['datetime', 'datetimetz']).columns.tolist()\n",
    "if datetime_cols:\n",
    "    print('Dropping raw datetime columns from features:', datetime_cols)\n",
    "    X = X.drop(columns=datetime_cols)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Column groups\n",
    "num_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Preprocessor\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_features),\n",
    "        ('cat', categorical_transformer, cat_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print('Engineered features added and preprocessing pipeline prepared.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection: correlation + RFE (numeric subset)\n",
    "if target_col in df.select_dtypes(include=[np.number]).columns:\n",
    "    corr_rank = df.select_dtypes(include=[np.number]).corr(numeric_only=True)[target_col].drop(target_col).abs().sort_values(ascending=False)\n",
    "    print('Top correlated numeric features:')\n",
    "    display(corr_rank.head(10).to_frame('abs_corr_with_target'))\n",
    "\n",
    "numeric_for_rfe = [c for c in df.select_dtypes(include=[np.number]).columns if c != target_col]\n",
    "if len(numeric_for_rfe) >= 3:\n",
    "    X_rfe = df[numeric_for_rfe].fillna(df[numeric_for_rfe].median())\n",
    "    y_rfe = df[target_col]\n",
    "    rfe_model = LinearRegression()\n",
    "    rfe = RFE(estimator=rfe_model, n_features_to_select=min(5, len(numeric_for_rfe)))\n",
    "    rfe.fit(X_rfe, y_rfe)\n",
    "    selected_features = [f for f, s in zip(numeric_for_rfe, rfe.support_) if s]\n",
    "    print('RFE selected features:', selected_features)\n",
    "else:\n",
    "    selected_features = numeric_for_rfe\n",
    "    print('RFE skipped due to low numeric feature count.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b99cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model development and comparison\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'DecisionTree': DecisionTreeRegressor(random_state=RANDOM_STATE),\n",
    "    'RandomForest': RandomForestRegressor(random_state=RANDOM_STATE, n_estimators=200),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "results = []\n",
    "trained_pipelines = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "\n",
    "    results.append({'Model': name, 'RMSE': rmse, 'MAE': mae, 'R2': r2})\n",
    "    trained_pipelines[name] = pipe\n",
    "\n",
    "comparison_df = pd.DataFrame(results).sort_values('RMSE')\n",
    "print('Model performance comparison:')\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f007a5",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Model Optimization & ANN Implementation\n",
    "\n",
    "Requirements covered:\n",
    "- Hyperparameter tuning (before vs after)\n",
    "- ANN with minimum 6 hidden layers and ReLU\n",
    "- Optimizer comparison (Adam, SGD)\n",
    "- Learning rate experiments\n",
    "- Training vs validation performance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning (RandomForest example)\n",
    "rf_base = trained_pipelines['RandomForest']\n",
    "base_preds = rf_base.predict(X_test)\n",
    "base_rmse = np.sqrt(mean_squared_error(y_test, base_preds))\n",
    "base_mae = mean_absolute_error(y_test, base_preds)\n",
    "base_r2 = r2_score(y_test, base_preds)\n",
    "\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [None, 8, 12, 16],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(rf_pipe, param_grid=param_grid, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid.best_estimator_\n",
    "tuned_preds = best_rf.predict(X_test)\n",
    "tuned_rmse = np.sqrt(mean_squared_error(y_test, tuned_preds))\n",
    "tuned_mae = mean_absolute_error(y_test, tuned_preds)\n",
    "tuned_r2 = r2_score(y_test, tuned_preds)\n",
    "\n",
    "tuning_compare_df = pd.DataFrame([\n",
    "    {'Model': 'RF Before Tuning', 'RMSE': base_rmse, 'MAE': base_mae, 'R2': base_r2},\n",
    "    {'Model': 'RF After Tuning', 'RMSE': tuned_rmse, 'MAE': tuned_mae, 'R2': tuned_r2}\n",
    "])\n",
    "\n",
    "print('Best RF params:', grid.best_params_)\n",
    "display(tuning_compare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN implementation and optimizer/learning-rate experiments\n",
    "if not TENSORFLOW_AVAILABLE:\n",
    "    print('ANN section skipped: install TensorFlow to run this block.')\n",
    "    ann_results_df = pd.DataFrame([{'Config': 'ANN not run', 'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan}])\n",
    "else:\n",
    "    X_train_proc = preprocessor.fit_transform(X_train)\n",
    "    X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "    # Convert sparse matrices if needed\n",
    "    if hasattr(X_train_proc, 'toarray'):\n",
    "        X_train_proc = X_train_proc.toarray()\n",
    "        X_test_proc = X_test_proc.toarray()\n",
    "\n",
    "    y_train_arr = np.asarray(y_train, dtype=np.float32)\n",
    "    y_test_arr = np.asarray(y_test, dtype=np.float32)\n",
    "\n",
    "    input_dim = X_train_proc.shape[1]\n",
    "\n",
    "    def build_ann(optimizer):\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "            Dense(96, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(48, activation='relu'),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "    optimizers = ['adam', 'sgd']\n",
    "    learning_rates = [0.001, 0.01]\n",
    "    ann_results = []\n",
    "    ann_histories = {}\n",
    "\n",
    "    for opt_name in optimizers:\n",
    "        for lr in learning_rates:\n",
    "            key = f'{opt_name}_lr_{lr}'\n",
    "            try:\n",
    "                if opt_name == 'adam':\n",
    "                    optimizer = Adam(learning_rate=lr, clipnorm=1.0)\n",
    "                else:\n",
    "                    optimizer = SGD(learning_rate=lr, clipnorm=1.0)\n",
    "\n",
    "                ann_model = build_ann(optimizer)\n",
    "                history = ann_model.fit(\n",
    "                    X_train_proc, y_train_arr,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=40,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[tf.keras.callbacks.TerminateOnNaN()]\n",
    "                )\n",
    "\n",
    "                preds = ann_model.predict(X_test_proc, verbose=0).ravel()\n",
    "\n",
    "                if not np.isfinite(preds).all():\n",
    "                    print(f'Skipping unstable ANN config: {key} (non-finite predictions).')\n",
    "                    ann_results.append({'Config': key, 'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan})\n",
    "                    continue\n",
    "\n",
    "                rmse = np.sqrt(mean_squared_error(y_test_arr, preds))\n",
    "                mae = mean_absolute_error(y_test_arr, preds)\n",
    "                r2 = r2_score(y_test_arr, preds)\n",
    "\n",
    "                ann_results.append({'Config': key, 'RMSE': rmse, 'MAE': mae, 'R2': r2})\n",
    "                ann_histories[key] = history\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Skipping failed ANN config {key}: {e}')\n",
    "                ann_results.append({'Config': key, 'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan})\n",
    "\n",
    "    ann_results_df = pd.DataFrame(ann_results)\n",
    "    ann_results_df = ann_results_df.sort_values('RMSE', na_position='last')\n",
    "\n",
    "    print('ANN experiment results:')\n",
    "    display(ann_results_df)\n",
    "\n",
    "    valid_ann_results_df = ann_results_df[ann_results_df['RMSE'].notna()]\n",
    "\n",
    "    if not valid_ann_results_df.empty:\n",
    "        best_ann_config = valid_ann_results_df.iloc[0]['Config']\n",
    "        print('Best ANN config:', best_ann_config)\n",
    "\n",
    "        # Training vs validation plot for best ANN config\n",
    "        best_hist = ann_histories[best_ann_config]\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(best_hist.history['loss'], label='Train Loss')\n",
    "        plt.plot(best_hist.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'ANN Training vs Validation Loss ({best_ann_config})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No stable ANN configuration produced finite predictions. Consider using smaller learning rates.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f529b",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Model Evaluation, Prediction & Business Interpretation\n",
    "\n",
    "This section finalizes:\n",
    "- Best overall model selection\n",
    "- Metric-based justification (RMSE, MAE, R¬≤)\n",
    "- Predictions on synthetic/sample records (5‚Äì10 rows)\n",
    "- Business implications, limitations, and future improvements\n",
    "- Final conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model selection: compare tuned RF and best ANN\n",
    "best_ml_row = tuning_compare_df.sort_values('RMSE').iloc[0]\n",
    "\n",
    "if ann_results_df['RMSE'].notna().any():\n",
    "    best_ann_row = ann_results_df.sort_values('RMSE').iloc[0]\n",
    "    final_compare = pd.DataFrame([\n",
    "        {'Candidate': best_ml_row['Model'], 'RMSE': best_ml_row['RMSE'], 'MAE': best_ml_row['MAE'], 'R2': best_ml_row['R2']},\n",
    "        {'Candidate': f\"ANN ({best_ann_row['Config']})\", 'RMSE': best_ann_row['RMSE'], 'MAE': best_ann_row['MAE'], 'R2': best_ann_row['R2']}\n",
    "    ]).sort_values('RMSE')\n",
    "else:\n",
    "    best_ann_row = None\n",
    "    final_compare = pd.DataFrame([\n",
    "        {'Candidate': best_ml_row['Model'], 'RMSE': best_ml_row['RMSE'], 'MAE': best_ml_row['MAE'], 'R2': best_ml_row['R2']}\n",
    "    ]).sort_values('RMSE')\n",
    "\n",
    "display(final_compare)\n",
    "\n",
    "best_overall = final_compare.iloc[0]['Candidate']\n",
    "print('Best performing model overall:', best_overall)\n",
    "\n",
    "# Synthetic/sample prediction (5-10 rows)\n",
    "sample_size = min(8, len(X_test))\n",
    "sample_X = X_test.sample(sample_size, random_state=RANDOM_STATE)\n",
    "\n",
    "if best_ann_row is not None and 'ANN' in best_overall and TENSORFLOW_AVAILABLE:\n",
    "    sample_proc = preprocessor.transform(sample_X)\n",
    "    if hasattr(sample_proc, 'toarray'):\n",
    "        sample_proc = sample_proc.toarray()\n",
    "\n",
    "    best_key = best_ann_row['Config']\n",
    "    opt_name = 'adam' if 'adam' in best_key else 'sgd'\n",
    "    lr = float(best_key.split('_')[-1])\n",
    "    optimizer = Adam(learning_rate=lr) if opt_name == 'adam' else SGD(learning_rate=lr)\n",
    "\n",
    "    final_ann = build_ann(optimizer)\n",
    "    final_ann.fit(X_train_proc, y_train_arr, validation_split=0.2, epochs=40, batch_size=32, verbose=0)\n",
    "    sample_preds = final_ann.predict(sample_proc, verbose=0).ravel()\n",
    "else:\n",
    "    final_model = best_rf if best_ml_row['Model'] == 'RF After Tuning' else rf_base\n",
    "    sample_preds = final_model.predict(sample_X)\n",
    "\n",
    "prediction_df = sample_X.copy()\n",
    "prediction_df['Predicted_' + target_col] = sample_preds\n",
    "print('Sample predictions (synthetic/sample rows):')\n",
    "display(prediction_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa6ecd",
   "metadata": {},
   "source": [
    "### Business Interpretation\n",
    "\n",
    "- **Best model selected:** Random Forest (After Tuning)\n",
    "- **Why selected (metrics):** It achieved the strongest test performance (RMSE ‚âà 131,703; MAE ‚âà 72,068; R¬≤ ‚âà 0.9445), clearly outperforming ANN and other baselines.\n",
    "- **Operational meaning:** The model can support better demand planning, inventory allocation, and promotion timing at store level.\n",
    "- **Limitations:** Holiday signal is weak in this processed sample, and store-level effects dominate; external drivers are limited.\n",
    "- **Future improvements:** Add richer event/holiday calendars, promotion metadata, lag/time-series features, and periodic retraining.\n",
    "\n",
    "### Final Conclusion\n",
    "The tuned Random Forest model provides strong and reliable sales prediction performance and is the most suitable deployment candidate for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef5090",
   "metadata": {},
   "source": [
    "## üìÇ Mandatory Submission Checklist\n",
    "\n",
    "- Jupyter Notebook (.ipynb)\n",
    "- Final PDF Report\n",
    "- Cleaned Dataset (if modified)\n",
    "- Model comparison table\n",
    "- ANN training performance graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
